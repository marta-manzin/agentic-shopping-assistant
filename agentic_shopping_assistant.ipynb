{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/marta-manzin/agentic-shopping-assistant/blob/main/agentic_shopping_assistant.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TOnQyK7obg_s"
   },
   "source": [
    "# üõí Agentic Shopping Assistant\n",
    "\n",
    "This notebook will go through all the steps to create an agentic shopping assistant. \\\n",
    "We will:\n",
    "1. Connect to OpenAI\n",
    "2. Create a simple agent\n",
    "3. Create an MCP server\n",
    "4. Create a LangGraph agent\n",
    "<br/>\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1to-6-8fnbAJ9bLTBWSf5buay6d2h94qw\" width=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4irdiFHM8Nz7"
   },
   "source": [
    "# ‚öôÔ∏è Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GBEKX8f4UE8F"
   },
   "source": [
    "Setup\n",
    "Before we start using OpenAI models, you need to set an API key. \\\n",
    "If you don't already have an key, you can generate one at: https://platform.openai.com/api-keys. \\\n",
    "Save the key as a Colab Secret variable called \"OPENAI_API_KEY\":\n",
    "1. Click on the key icon in the left bar menu.\n",
    "2. Click on `+ Add new secret`.\n",
    "3. Name the variable and paste the key in the value field.\n",
    "4. Enable notebook access.\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1lMPgLbeqZ1lxYMQwbe5F3n9Qko4u55FH\" width=\"450\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kZxTvnYm8Q1O"
   },
   "source": [
    "Let's test it. First, import the key into the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "try:\n",
    "  from google.colab import userdata\n",
    "  os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
    "except:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('IN_JUPYTER:', True)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Where are we running\n",
    "from IPython import get_ipython\n",
    "if get_ipython() is not None:\n",
    "    IN_JUPYTER = True\n",
    "else:\n",
    "    IN_JUPYTER = False\n",
    "(\"IN_JUPYTER:\", IN_JUPYTER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "aIdi2xjLbWOX",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Then, make a test call to OpenAI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FvlxGVjz8S7l",
    "outputId": "59d7e89b-f3f3-4eef-c05c-43fe696cf0a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM test: OK\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "client = openai.OpenAI()\n",
    "model = \"gpt-4o\"\n",
    "\n",
    "# Test that the LLM is set up correctly\n",
    "response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Say 'OK' if you can read this.\"}],\n",
    "    max_tokens=10\n",
    ")\n",
    "print(f\"LLM test: {response.choices[0].message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ToWYG3qNDzH"
   },
   "source": [
    "# ü§ñ Creating an Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ATPYxIUbIigj"
   },
   "source": [
    "In Python, a set is an unordered collection of unique elements. \\\n",
    "We will build an agent that adds and removes strings from a set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ujra-LLDcaeP"
   },
   "source": [
    "The System Prompt gives some context to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "u2p8auWt_fGE"
   },
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a helpful assistant that adds and removes strings from a set.\n",
    "\n",
    "You have access to tools that let you:\n",
    "1. Add a string, if it is not already in the set.\n",
    "2. Remove a string.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XwzdXKyycs0P"
   },
   "source": [
    "Here are the available tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "eM8NowbiOv74"
   },
   "outputs": [],
   "source": [
    "MY_SET = set()\n",
    "\n",
    "def insertion_tool(s: str):\n",
    "  \"\"\"Tool: Add a string to a set.\"\"\"\n",
    "  MY_SET.add(s)\n",
    "\n",
    "def removal_tool(s: str):\n",
    "  \"\"\"Tool: Remove a string from a set.\"\"\"\n",
    "  if s in MY_SET:\n",
    "    MY_SET.remove(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AOlTPB1RcVDH"
   },
   "source": [
    "Provide a description of each tool to the LLM. \\\n",
    "The LLM will use it to decide which tools to call and with what arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true,
    "id": "qY3nlJhU-x7U",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"insertion_tool\",\n",
    "            \"description\": \"Add a string to a set.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"s\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The string to be added.\"\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"s\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"removal_tool\",\n",
    "            \"description\": \"Add a string to a set.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"s\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The string to be removed.\"\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"s\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FTe3UobJc8DF"
   },
   "source": [
    "If the LLM decides to run a tool, it will respond with a \"tool call\" object. \\\n",
    "A tool call looks like this:\n",
    "\n",
    "```\n",
    "{\n",
    "  id: <unique-id>,\n",
    "  function: {\n",
    "    arguments: '{\"s\":\"my_string\"}',\n",
    "    name: 'insertion_tool'\n",
    "  },\n",
    "  type: 'function'\n",
    "}\n",
    "```\n",
    "\n",
    "The following code parses a tool call and runs the tool.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true,
    "id": "u5YkPNiU_iKR",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def execute(tool_call) -> str:\n",
    "    \"\"\"Execute a tool call and return the result, if any.\"\"\"\n",
    "    # Extract the function name from the tool call\n",
    "    function_name = tool_call.function.name\n",
    "\n",
    "    # Parse the arguments from JSON string to dictionary\n",
    "    arguments = json.loads(tool_call.function.arguments)\n",
    "\n",
    "    # Look up the function by name in the global scope\n",
    "    tool_func = globals().get(function_name)\n",
    "\n",
    "    # Check if the function exists and is callable\n",
    "    if tool_func is None or not callable(tool_func):\n",
    "        return f\"Unknown function: {function_name}\"\n",
    "\n",
    "    # Call the function with the unpacked arguments\n",
    "    response = tool_func(**arguments)\n",
    "\n",
    "    # Return the result of the function call, if any\n",
    "    if response:\n",
    "      return str(response)\n",
    "    else:\n",
    "      return \"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QyOsu-4tleGP"
   },
   "source": [
    "And last, the agent logic. \\\n",
    "Instead of using a ready-made framework, the code below does *direct orchestration*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "u4B9C8CKXmOm"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def submit_request(\n",
    "    user_prompt: str,\n",
    "    verbose: bool = True\n",
    "    ):\n",
    "    \"\"\"Submit a request to the agent and run any tools it calls.\"\"\"\n",
    "    # Initialize the chat history\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "\n",
    "    for iteration in itertools.count(1):\n",
    "\n",
    "        # Ask the agent what to do next\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            tools=tools,\n",
    "            tool_choice=\"auto\"\n",
    "        ).choices[0].message\n",
    "\n",
    "        # Update the chat history with the agent's response\n",
    "        messages.append({\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": response.content,\n",
    "            \"tool_calls\": response.tool_calls\n",
    "        })\n",
    "\n",
    "        # If agent did not call any tools, we are done\n",
    "        if not response.tool_calls:\n",
    "            if verbose:\n",
    "              print(f\"\\n‚≠ê The resulting set is: {MY_SET}\")\n",
    "            break\n",
    "\n",
    "        # Execute all tool calls\n",
    "        for tool_call in response.tool_calls:\n",
    "            if verbose:\n",
    "              print(f\"\\nüîß The agent is calling a tool: \"\n",
    "                  f\"{tool_call.function.name}\"\n",
    "                  f\"({json.loads(tool_call.function.arguments)})\")\n",
    "\n",
    "            outcome = execute(tool_call)\n",
    "            messages.append({\n",
    "                \"role\": \"tool\",\n",
    "                \"tool_call_id\": tool_call.id,\n",
    "                \"content\": str(outcome)\n",
    "            })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NF26AUcTeM9C"
   },
   "source": [
    "Let's test the agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tfQWYK7ceHVY",
    "outputId": "5a260f5c-ba5c-4b83-fa20-d763911a678a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîß The agent is calling a tool: insertion_tool({'s': 'apples'})\n",
      "\n",
      "üîß The agent is calling a tool: insertion_tool({'s': 'oranges'})\n",
      "\n",
      "üîß The agent is calling a tool: insertion_tool({'s': 'pears'})\n",
      "\n",
      "‚≠ê The resulting set is: {'apples', 'pears', 'oranges'}\n"
     ]
    }
   ],
   "source": [
    "submit_request(\"Please add 'apples', 'oranges' and 'pears' to the set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dnCP2npeeYev",
    "outputId": "b3d7b53a-00c0-48ec-eb80-a13ed9ece4dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîß The agent is calling a tool: removal_tool({'s': 'oranges'})\n",
      "\n",
      "‚≠ê The resulting set is: {'apples', 'pears'}\n"
     ]
    }
   ],
   "source": [
    "submit_request(\"Please remove 'oranges' from the set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W_P3GEVCNTQ6"
   },
   "source": [
    "# üóÑÔ∏è Creating an MCP Server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rD6I4qqM3-K-"
   },
   "source": [
    "Create the MCP server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xem5Szb0AQqv",
    "outputId": "f8e8d00c-7514-41e9-9b11-dc2d53e0acb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "‚úì Server created\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet mcp\n",
    "from mcp.server import Server\n",
    "from mcp.types import Tool, TextContent\n",
    "\n",
    "server = Server(\"set-server\")\n",
    "print(\"‚úì Server created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PuRlZnAYATtj"
   },
   "source": [
    "Create an MCP wrapper for listing the available tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 103
    },
    "id": "R4RQ-E6QArjt",
    "outputId": "93c8a3c4-94a1-49e3-c0f3-c3e72f11160e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.list_tools() -> list[mcp.types.Tool]>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "async def list_tools() -> list[Tool]:\n",
    "    \"\"\"Return the list of available tools from our tools definition.\"\"\"\n",
    "    # Create an empty list to store MCP Tool objects\n",
    "    mcp_tools = []\n",
    "\n",
    "    # Convert each tool from our OpenAI format to MCP format\n",
    "    for tool_def in tools:\n",
    "        # Extract the function definition from the OpenAI tool format\n",
    "        func_def = tool_def[\"function\"]\n",
    "\n",
    "        # Create an MCP Tool object with the same information\n",
    "        mcp_tools.append(Tool(\n",
    "            name=func_def[\"name\"], # the function name\n",
    "            description=func_def[\"description\"], # what the tool does\n",
    "            inputSchema=func_def[\"parameters\"] # the JSON schema for parameters\n",
    "        ))\n",
    "\n",
    "    # Return the list of MCP Tool objects\n",
    "    return mcp_tools\n",
    "\n",
    "# Register the list_tools function with the server\n",
    "# This tells the MCP server to use this function when clients ask for available tools\n",
    "server.list_tools()(list_tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NM4w_xtuAvHM"
   },
   "source": [
    "Create an MCP wrapper for executing tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 103
    },
    "id": "ib0oaEEQBJp2",
    "outputId": "38fbf6af-0db0-42d9-90fe-2d074d1e9ec4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.call_tool(name: str, arguments: dict) -> list[mcp.types.TextContent]>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "async def call_tool(name: str, arguments: dict) -> list[TextContent]:\n",
    "    \"\"\"Handle MCP tool calls by delegating to our existing tools.\"\"\"\n",
    "    # Convert MCP format to the format expected by execute()\n",
    "    # execute() expects: tool_call.function.name and tool_call.function.arguments\n",
    "\n",
    "    # Create the inner function object with name and arguments\n",
    "    function = SimpleNamespace(\n",
    "        name=name,\n",
    "        arguments=json.dumps(arguments)  # Convert dict to JSON string\n",
    "    )\n",
    "\n",
    "    # Create the tool_call object with the function attribute\n",
    "    tool_call = SimpleNamespace(function=function)\n",
    "\n",
    "    # Execute the tool using our existing execute() function\n",
    "    result = execute(tool_call)\n",
    "\n",
    "    # Convert result to MCP response format\n",
    "    result_text = str(result) if result is not None else \"Success\"\n",
    "    return [TextContent(type=\"text\", text=result_text)]\n",
    "\n",
    "# Register the call_tool function with the server\n",
    "server.call_tool()(call_tool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5_UTk9kMBXdv"
   },
   "source": [
    "Expose an HTTP/SSE endpoint for the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BJ57ifFcBbDx",
    "outputId": "a430872a-860f-411b-f7c3-9bf02c745ee2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "‚úì FastAPI app created\n"
     ]
    }
   ],
   "source": [
    "# FastAPI is a framework for building REST APIs\n",
    "%pip install --quiet fastapi\n",
    "from mcp.server.sse import SseServerTransport\n",
    "from fastapi import FastAPI, Request\n",
    "from fastapi.responses import Response\n",
    "\n",
    "# Create an SSE transport that will handle messages at the \"/messages\" path\n",
    "sse = SseServerTransport(\"/messages\")\n",
    "\n",
    "# Create a FastAPI web application\n",
    "app = FastAPI()\n",
    "\n",
    "\n",
    "async def handle_sse(request: Request):\n",
    "    \"\"\"Handle incoming SSE connections from MCP clients.\"\"\"\n",
    "    # Connect the SSE transport to get read/write streams\n",
    "    async with sse.connect_sse(\n",
    "        request.scope, request.receive, request._send\n",
    "    ) as (read_stream, write_stream):\n",
    "        # Run the MCP server with these streams\n",
    "        await server.run(\n",
    "            read_stream,\n",
    "            write_stream,\n",
    "            server.create_initialization_options()\n",
    "        )\n",
    "    return Response()\n",
    "\n",
    "# Register the GET endpoint with the FastAPI app\n",
    "# Clients connect to http://host:port/sse to establish SSE connection\n",
    "app.add_api_route(\"/sse\", handle_sse, methods=[\"GET\"])\n",
    "\n",
    "# Mount the POST handler for receiving messages\n",
    "# Clients send messages to http://host:port/messages\n",
    "app.mount(\"/messages\", sse.handle_post_message)\n",
    "\n",
    "print(\"‚úì FastAPI app created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v1Sp7zRqBjV7"
   },
   "source": [
    "Start the MCP server in the background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "‚úì Starting MCP HTTP server on port 49691 in background...\n",
      "  Server available at http://127.0.0.1:49691/sse\n"
     ]
    }
   ],
   "source": [
    "# Uvicorn is a web server that handles HTTP requests and asynchronous code\n",
    "%pip install --quiet uvicorn\n",
    "import threading\n",
    "import uvicorn\n",
    "import sys\n",
    "import random\n",
    "\n",
    "# The port number where the server will listen\n",
    "server_port = random.randint(49152, 65535)\n",
    "\n",
    "def run_server():\n",
    "    \"\"\"Run the uvicorn server. This will be called in a background thread.\"\"\"\n",
    "    try:\n",
    "        # Start the server on all network interfaces (0.0.0.0) at the specified port\n",
    "        uvicorn.run(app, host=\"0.0.0.0\", port=server_port, log_level=\"warning\")\n",
    "    except Exception as e:\n",
    "        # Print any errors to stderr\n",
    "        print(f\"‚úó Server error: {e}\", file=sys.stderr)\n",
    "\n",
    "# Start server in background thread\n",
    "server_thread = threading.Thread(\n",
    "    target=run_server, # thread will automatically stop when main program exits\n",
    "    daemon=True\n",
    "  )\n",
    "server_thread.start()\n",
    "\n",
    "print(f\"‚úì Starting MCP HTTP server on port {server_port} in background...\")\n",
    "print(f\"  Server available at http://127.0.0.1:{server_port}/sse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YFpcTtJWB2-W"
   },
   "source": [
    "Verify that the server port is open and listening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VnEligIZB4qz",
    "outputId": "d9d4e412-3bd0-437e-aefc-5519f5c43870"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Attempt 1/5: Port 49691 not ready yet...\n",
      "‚úì Port 49691 is open (attempt 2/5)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import socket\n",
    "\n",
    "sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "sock.settimeout(1)\n",
    "\n",
    "# Try up to 5 times to verify the server started successfully\n",
    "for attempt in range(1, 6):\n",
    "    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    sock.settimeout(1)\n",
    "    \n",
    "    try:\n",
    "        sock.connect((\"127.0.0.1\", server_port))\n",
    "        print(f\"‚úì Port {server_port} is open (attempt {attempt}/5)\")\n",
    "        sock.close()\n",
    "        break   # Exit the loop early since we confirmed the server is running\n",
    "    except: \n",
    "        print(f\"‚è≥ Attempt {attempt}/5: Port {server_port} not ready yet...\")\n",
    "        # Wait 1 second before trying again (unless this is the last attempt)\n",
    "        if attempt < 5:\n",
    "            time.sleep(1)\n",
    "\n",
    "else:\n",
    "    # This else block runs if we never broke out of the loop (all 5 attempts failed)\n",
    "    print(f\"‚úó Port {server_port} is not open after 5 attempts\")\n",
    "    print(\"Make sure the server is running (previous cell)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6l0hKhJyCB7s"
   },
   "source": [
    "Test the server with a dummy client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "id": "QQk64JgCvFrP",
    "outputId": "1ce97827-bf5b-4ff3-fa62-2975ebb941f5",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mcp import ClientSession\n",
    "from mcp.client.sse import sse_client\n",
    "\n",
    "# Build the URL where our server is listening\n",
    "server_url = f\"http://127.0.0.1:{server_port}/sse\"\n",
    "\n",
    "async def test_client():\n",
    "    \"\"\"Test that the MCP server works by calling tools as a client.\"\"\"\n",
    "    # Connect to the server using SSE client\n",
    "    async with sse_client(server_url) as (read, write):\n",
    "        # Create a client session with the read/write streams\n",
    "        async with ClientSession(read, write) as session:\n",
    "            # Initialize the session (required handshake)\n",
    "            await session.initialize()\n",
    "\n",
    "            # List available tools from the server\n",
    "            available_tools = await session.list_tools()\n",
    "            print(\"Available tools:\", [t.name for t in available_tools.tools])\n",
    "\n",
    "            # Test the insertion_tool by adding 'cherries' to the set\n",
    "            print(\"\\nTesting insertion_tool with 'cherries':\")\n",
    "            result = await session.call_tool(\"insertion_tool\", {\"s\": \"cherries\"})\n",
    "            print(\"Result:\", result.content[0].text)\n",
    "            print(\"Current set:\", MY_SET)\n",
    "\n",
    "            # Test the removal_tool by removing 'cherries' from the set\n",
    "            print(\"\\nTesting removal_tool with 'cherries':\")\n",
    "            result = await session.call_tool(\"removal_tool\", {\"s\": \"cherries\"})\n",
    "            print(\"Result:\", result.content[0].text)\n",
    "            print(\"Current set:\", MY_SET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# I had to separate out the runing of the async functions at the top level because what works in jupyter\n",
    "# doesn't work in straight python.  And vice versa.  I will remove the day before class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available tools: ['insertion_tool', 'removal_tool']\n",
      "\n",
      "Testing insertion_tool with 'cherries':\n",
      "Result: \n",
      "Current set: {'cherries', 'apples', 'pears'}\n",
      "\n",
      "Testing removal_tool with 'cherries':\n",
      "Result: \n",
      "Current set: {'apples', 'pears'}\n"
     ]
    }
   ],
   "source": [
    "# Run the async test function\n",
    "await test_client()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": [
     "active-py"
    ]
   },
   "source": [
    "# py version\n",
    "import asyncio\n",
    "if not IN_JUPYTER:\n",
    "    asyncio.run(test_client())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "hhA0L8dUNvYJ",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# üß† Orchestration with LangGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qrIbDfYJN3y0",
    "outputId": "2f685b79-a59b-4f6d-9253-5f1f871ae4a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip uninstall -y -qqq langchain  \n",
    "%pip install --quiet \"langchain-openai>=0.2,<1.0\" \"langchain_mcp_adapters\" \"langgraph\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "id": "5x5y8R-nkMrq",
    "outputId": "e9218aef-d93e-4a0a-8420-51855447b3fc",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "\n",
    "# Create MCP client that connects to your set-server\n",
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"set-server\": {\n",
    "            \"transport\": \"sse\",\n",
    "            \"url\": f\"http://localhost:{server_port}/sse\",\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Loaded 2 tools from MCP server\n",
      "  - insertion_tool: Add a string to a set.\n",
      "  - removal_tool: Add a string to a set.\n"
     ]
    }
   ],
   "source": [
    "# Get available tools from the MCP server\n",
    "tools_from_mcp = await client.get_tools()\n",
    "print(f\"‚úì Loaded {len(tools_from_mcp)} tools from MCP server\")\n",
    "for tool in tools_from_mcp:\n",
    "    print(f\"  - {tool.name}: {tool.description}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not IN_JUPYTER:\n",
    "    tools_from_mcp = asyncio.run(client.get_tools())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "id": "t_p_I0JokOjL",
    "outputId": "fb80bb10-ac50-4e5c-c93f-f744e81fae33",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì LangGraph agent created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tz/lnl7pfyn7cs4bxvy7vqk4h8sbyfpcy/T/ipykernel_82466/1026815421.py:5: LangGraphDeprecatedSinceV10: create_react_agent has been moved to `langchain.agents`. Please update your import to `from langchain.agents import create_agent`. Deprecated in LangGraph V1.0 to be removed in V2.0.\n",
      "  agent_executor = create_react_agent(\n"
     ]
    }
   ],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Create a LangGraph agent using the tools we already loaded\n",
    "agent_executor = create_react_agent(\n",
    "    ChatOpenAI(model=\"gpt-4o\", temperature=0),\n",
    "    tools_from_mcp,\n",
    ")\n",
    "\n",
    "print(\"‚úì LangGraph agent created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "result = await agent_executor.ainvoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Please add 'grapes', 'kiwi', and 'mango' to the set.\"}]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not IN_JUPYTER:\n",
    "    result = asyncio.run(\n",
    "        agent_executor.ainvoke({\n",
    "            \"messages\": [{\n",
    "                \"role\": \"user\", \n",
    "                \"content\": \"Please add 'grapes', 'kiwi', and 'mango' to the set.\"}]\n",
    "        })\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "id": "7zu7HEPAkQlc",
    "outputId": "637a962e-c096-4f8e-8671-0cf1b49f936d",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: \u001b[32mPlease add 'grapes', 'kiwi', and 'mango' to the set.\u001b[0m\n",
      "Tool Result: \u001b[32mSuccess\u001b[0m\n",
      "Tool Result: \u001b[32mSuccess\u001b[0m\n",
      "Tool Result: \u001b[32mSuccess\u001b[0m\n",
      "AI: \u001b[34mThe items 'grapes', 'kiwi', and 'mango' have been successfully added to the set.\u001b[0m\n",
      "\n",
      "‚≠ê The resulting set is: {'pears', 'grapes', 'mango', 'apples', 'kiwi'}\n"
     ]
    }
   ],
   "source": [
    "# Display the conversation\n",
    "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage\n",
    "\n",
    "for message in result['messages']:\n",
    "    if isinstance(message, HumanMessage):\n",
    "        print(\"Human: \\033[32m\" + message.content + \"\\033[0m\")\n",
    "    elif isinstance(message, AIMessage):\n",
    "        if message.content:\n",
    "            print(\"AI: \\033[34m\" + message.content + \"\\033[0m\")\n",
    "    elif isinstance(message, ToolMessage):\n",
    "        if \"Error\" not in message.content:\n",
    "            print(f\"Tool Result: \\033[32mSuccess\\033[0m\")\n",
    "\n",
    "print(f\"\\n‚≠ê The resulting set is: {MY_SET}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "K0Cw_2GpvqKp",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# üßπ Cleanup\n",
    "\n",
    "Stop the MCP server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "7kTywUxOwKua"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Server stopped\n"
     ]
    }
   ],
   "source": [
    "# Kill any process running uvicorn on our server port\n",
    "!pkill -f \"uvicorn.*{server_port}\"\n",
    "print(\"‚úì Server stopped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dvwl7EKJTZSL"
   },
   "source": [
    "# Thank you!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "H4N9Oqg7No6i",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "###"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMY3QOlUkeW6Eyu3ASfcuXu",
   "include_colab_link": true,
   "provenance": []
  },
  "jupytext": {
   "cell_metadata_filter": "tags,outputId,id,colab_type,-editable,-slideshow,-colab",
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
